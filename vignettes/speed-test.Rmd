---
title: "Speed testing Gaussian predictive process and nearest neighbor Gaussian process models"
author: "Sean C. Anderson and Eric J. Ward"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Spatial GLMs with glmmfields}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r set-knitr-options, cache=FALSE, echo=FALSE}
library("knitr")
opts_chunk$set(message = FALSE, fig.width = 5.5)
```

Let's load the necessary packages:

```{r, message=FALSE, warning=FALSE}
library(glmmfields)
library(ggplot2)
library(dplyr)
```

Set up parallel processing (not used in this example):

```{r, echo=TRUE, eval=FALSE}
options(mc.cores = parallel::detectCores())
```

The spatial GLMMs estimated in `glmmfields()` are powerful, but may be limited for very large datasets. Specifically, estimation of these models requires inverting the covariance matrix of the random effects at the know locations. To illustrate this, we'll first simulate some data. We will use the built-in function `sim_glmmfields()`, but normally you would start with your own data. We will simulate 1000 data points, some (fake) an underlying random field spatial pattern, and add some observation error. We'll simulate a fairly complex spatial surface, with 75 knots.

```{r simulate-data}
set.seed(123)
N <- 1000 # number of data points
s <- sim_glmmfields(
  n_draws = 5, gp_theta = 1.5, n_data_points = N,
  gp_sigma = 0.2, sd_obs = 0.2, n_knots = 75, obs_error = "normal",
  covariance = "squared-exponential"
)
d <- s$dat

ggplot(s$dat, aes(lon, lat, colour = y)) +
  viridis::scale_colour_viridis() +
  geom_point(size = 3)
```

To illustrate how the spatial GLMM model slows down as the number of knots increases, we can fit a series of models with varying numbers of knots and record the time required to sample 100 iterations. Note that we are only using 1 chain and 500 iterations here so the vignette builds quickly on CRAN. For final inference, you should likely use 4 or more chains and 2000 or more iterations.

```{r fit-models, results='hide', eval=FALSE}
df = data.frame("knots"=seq(10,100,by=10), "time"=NA)
for(i in 1:nrow(df)) {
start_time <- Sys.time()
  m_spatial <- glmmfields(y ~ 1,
  data = d,
  lat = "lat", lon = "lon", nknots = df$knots[i], iter = 100, chains = 1,
  prior_intercept = student_t(3, 0, 10), 
  prior_beta = student_t(3, 0, 3),
  prior_sigma = half_t(3, 0, 3),
  prior_gp_theta = half_t(3, 0, 10),
  prior_gp_sigma = half_t(3, 0, 3),
  seed = 123 # passed to rstan::sampling()
)
end_time <- Sys.time()
df$time[i] = as.numeric(difftime(end_time,start_time,units="secs"))

}
```

```{r echo=FALSE}
df = data.frame("knots"=seq(10,100,by=10), "time"=c(17,24,58,61,74,87,167,205,303,457))
```

Let's plot the time (seconds) as a function of knots,

```{r knots-v-time}
ggplot(df, aes(knots, time)) + geom_line() + geom_point() + ylab("Seconds") + xlab("Knots")
```

We can compare the model above with a Nearest Neighbor Gaussian Process model, which doesn't require the number of knots to be specified. Instead, this approach uses the number of nearest neighbors used in the estimation of each point, and finishes in about 2 minutes. 

```{r nngp-model, eval = FALSE}
start_time <- Sys.time()
m_spatial_gp <- glmmfields(y ~ 1,
  data = d,
  lat = "lat", lon = "lon", iter = 1000, chains = 1,
  prior_intercept = student_t(3, 0, 10), 
  prior_beta = student_t(3, 0, 3),
  prior_sigma = half_t(3, 0, 3),
  prior_gp_theta = half_t(3, 0, 10),
  prior_gp_sigma = half_t(3, 0, 3),
  seed = 123,
  method="NNGP",
  nngp_neighbors = 5
)
end_time <- Sys.time()
diff = as.numeric(difftime(end_time,start_time,units="secs"))
```
